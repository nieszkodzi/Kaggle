{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Landmark Recognition - data preparation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nieszkodzi/Kaggle/blob/master/Landmark_Recognition_data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "65F17Z4RJnGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d91fe7cf-21b3-41bd-c521-e56b08cea71a"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.applications import MobileNet\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, AveragePooling2D, Dropout\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "XtI-hAG0JnGx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data_dir = 'images/train/'\n",
        "validation_data_dir = 'images/validation/'\n",
        "\n",
        "batch_size = 16\n",
        "img_height, img_width = 160, 160"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDZ7JDSLJnG-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "6597a82f-da5c-4017-939f-6e1b96b9cadf"
      },
      "cell_type": "code",
      "source": [
        "# Pixel values rescaling from [0, 255] to [0, 1] interval\n",
        "train_generator = ImageDataGenerator(rescale=1/255, rotation_range=90, zoom_range=0.2, horizontal_flip=True)\n",
        "\n",
        "validation_generator = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Retrieve images and their classes for train and validation sets\n",
        "train_flow = train_generator.flow_from_directory(directory=train_data_dir, \n",
        "                                                 batch_size=batch_size, target_size=(img_height,img_width), \n",
        "                                                 class_mode= 'binary')\n",
        "\n",
        "validation_flow = validation_generator.flow_from_directory(\n",
        "    directory=validation_data_dir, batch_size=batch_size, \n",
        "    target_size=(img_height,img_width), class_mode= 'binary')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f140686f8fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m train_flow = train_generator.flow_from_directory(directory=train_data_dir, \n\u001b[1;32m      7\u001b[0m                                                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                                  class_mode= 'binary')\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m validation_flow = validation_generator.flow_from_directory(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         )\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images/train/'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "BpLambO-JnHK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_train_samples = 9893\n",
        "nb_validation_samples = 1969\n",
        "n_classes =  10\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_fo4wOPkJnHS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Basic CNN: Model architecture definition\n",
        "\n",
        "We build a sequential model and add convolutional layers and max pooling layers to it. We also add dropout layers in between, dropout randomly switches off some neurons in the network which forces the data to find new paths. Therefore, this reduces overfitting. We add dense layers at the end which are used for class prediction(1â€“13).\n",
        "\n",
        "Max-pooling: A technique used to reduce the dimensions of an image by taking the maximum pixel value of a grid. This also helps reduce overfitting and makes the model more generic. The example below show how 2 x 2 max pooling works"
      ]
    },
    {
      "metadata": {
        "id": "chf_02L6JnHW",
        "colab_type": "code",
        "colab": {},
        "outputId": "4f52c178-7444-4d6f-ac3a-38356e71c3eb"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation= 'softmax'))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "            optimizer=\"adam\",\n",
        "            metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# Model saving callback\n",
        "checkpointer = ModelCheckpoint(filepath='basic_cnn.h5py', monitor='val_acc', save_best_only=True)\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_acc', verbose=1, patience=5)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 158, 158, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 79, 79, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 77, 77, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 38, 38, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 36, 36, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 18, 18, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 20736)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               10617344  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 10,646,497\n",
            "Trainable params: 10,646,497\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PxUvrTAYJnHn",
        "colab_type": "code",
        "colab": {},
        "outputId": "c33a481f-07dd-4f14-8e36-d55299c0d703"
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(generator=train_flow, \n",
        "                    samples_per_epoch=nb_train_samples,\n",
        "                    validation_data=validation_flow,\n",
        "                    nb_val_samples=nb_validation_samples,\n",
        "                    callbacks=[checkpointer, early_stopping],\n",
        "                    verbose=1,\n",
        "                    epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/natalia/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  import sys\n",
            "/home/natalia/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<keras_pre..., validation_data=<keras_pre..., callbacks=[<keras.ca..., verbose=1, epochs=10, steps_per_epoch=618, validation_steps=1969)`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "618/618 [==============================] - 2309s 4s/step - loss: -55.8870 - acc: 0.1001 - val_loss: -55.8139 - val_acc: 0.1001\n",
            "Epoch 2/10\n",
            "618/618 [==============================] - 2300s 4s/step - loss: -55.8358 - acc: 0.0999 - val_loss: -55.7606 - val_acc: 0.0999\n",
            "Epoch 3/10\n",
            "618/618 [==============================] - 2305s 4s/step - loss: -55.8735 - acc: 0.1003 - val_loss: -55.8463 - val_acc: 0.0999\n",
            "Epoch 4/10\n",
            "618/618 [==============================] - 2307s 4s/step - loss: -55.8605 - acc: 0.0998 - val_loss: -55.7494 - val_acc: 0.1006\n",
            "Epoch 5/10\n",
            "618/618 [==============================] - 2310s 4s/step - loss: -55.8822 - acc: 0.1004 - val_loss: -55.8198 - val_acc: 0.0996\n",
            "Epoch 6/10\n",
            "618/618 [==============================] - 2326s 4s/step - loss: -55.8254 - acc: 0.1004 - val_loss: -55.7948 - val_acc: 0.1003\n",
            "Epoch 7/10\n",
            "618/618 [==============================] - 2324s 4s/step - loss: -55.9247 - acc: 0.0997 - val_loss: -55.7290 - val_acc: 0.1000\n",
            "Epoch 8/10\n",
            "618/618 [==============================] - 2304s 4s/step - loss: -55.9840 - acc: 0.1003 - val_loss: -55.8519 - val_acc: 0.1000\n",
            "Epoch 9/10\n",
            "618/618 [==============================] - 2293s 4s/step - loss: -55.9799 - acc: 0.0989 - val_loss: -55.8057 - val_acc: 0.0999\n",
            "Epoch 00009: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdc4070a7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "7u6JQLybJnH4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BfXsn80ZJnIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vys9Du3wJnIX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# "
      ]
    }
  ]
}